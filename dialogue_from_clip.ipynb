{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "import torch\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"openai_api_key.txt\", \"r\") as f: \n",
    "    openai_api_key = f.read().strip()\n",
    "with open(\"huggingface_token.txt\", \"r\") as f:\n",
    "    huggingface_token = f.read().strip()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueExtractor:\n",
    "    def __init__(self, audio_clip_path, transcription_type='api'):\n",
    "        self.audio_path = audio_clip_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.speaker_diarization_model = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=huggingface_token\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if transcription_type == 'api':\n",
    "            self.processor = None\n",
    "            self.local_transcriber = None\n",
    "        elif transcription_type == 'local':\n",
    "            self.processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "            self.local_transcriber = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\").to(self.device)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid transcription type. Choose either 'api' or 'local'\")\n",
    "        \n",
    "        self.waveform, self.sample_rate = torchaudio.load(self.audio_path)\n",
    "    \n",
    "    def _diarize(self):\n",
    "        output = self.speaker_diarization_model({'waveform': self.waveform, 'sample_rate': self.sample_rate}, \n",
    "                                                min_speakers=2)\n",
    "        segments = list(output.itersegments())\n",
    "        \n",
    "        diarization = pd.DataFrame(columns=['start', 'end', 'speaker'])\n",
    "        for segment in segments:\n",
    "            speakers = output.get_labels(segment)\n",
    "            for speaker in speakers:\n",
    "                start = segment.start\n",
    "                end = segment.end\n",
    "                \n",
    "                diarization.loc[len(diarization)] = [start, end, speaker]\n",
    "\n",
    "        return diarization\n",
    "\n",
    "    def _transcribe_api(self, clip_path):\n",
    "        with open(clip_path, \"rb\") as audio_file:\n",
    "            transcription = openai.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file,\n",
    "                language=\"en\"\n",
    "            )\n",
    "\n",
    "        return transcription.text\n",
    "\n",
    "    def _transcribe_local(self, audio_path):\n",
    "        wav, sr = torchaudio.load(audio_path)\n",
    "        wav = wav.mean(dim=0).numpy()\n",
    "        inputs = self.processor(wav, return_tensors=\"pt\", sampling_rate=self.sample_rate).to(self.device)\n",
    "        input_features = inputs.input_features\n",
    "        seq = self.local_transcriber.generate(inputs=input_features)\n",
    "\n",
    "        transcription = self.processor.batch_decode(seq, skip_special_tokens=True)[0]\n",
    "\n",
    "        return transcription\n",
    "    \n",
    "    def _transcribe(self, audio_path):\n",
    "        if self.local_transcriber is not None:\n",
    "            return self._transcribe_local(audio_path)\n",
    "        else:\n",
    "            return self._transcribe_api(audio_path)\n",
    "\n",
    "    def extract_dialogue(self):\n",
    "        diarization_df = self._diarize()\n",
    "        \n",
    "        transcription_df = diarization_df.copy()\n",
    "        transcription_df['transcription'] = None\n",
    "        for i in range(len(diarization_df)):\n",
    "            start = diarization_df.loc[i, 'start']\n",
    "            end = diarization_df.loc[i, 'end']\n",
    "            \n",
    "            wav = self.waveform[:, int(start*self.sample_rate) : int(end*self.sample_rate)].clone()\n",
    "            \n",
    "            tmp_path = f\"temp_{i}.wav\"\n",
    "            torchaudio.save(tmp_path, wav, self.sample_rate)\n",
    "            transcription = self._transcribe(tmp_path)\n",
    "            # os.remove(tmp_path)\n",
    "\n",
    "            transcription_df.loc[i, 'transcription'] = transcription\n",
    "        \n",
    "        return transcription_df\n",
    "\n",
    "def extract_clip_from_full(full_vid_path, clip_path, secs, duration):\n",
    "    print(f\"Extracting clip from {full_vid_path} to {clip_path}\")\n",
    "    cap = cv2.VideoCapture(full_vid_path)\n",
    "    print(f\"Is opened?: {cap.isOpened()}\")\n",
    "    \n",
    "    clip = VideoFileClip(full_vid_path)\n",
    "    subclip = clip.subclip(secs, secs+duration)\n",
    "    subclip.write_videofile(clip_path, fps=5, codec='libx264', audio=True, audio_fps=100*160, audio_codec=\"pcm_s16le\", verbose=False, logger=None)\n",
    "    print(f\"Video clip saved at {clip_path}\")\n",
    "    print()\n",
    "    \n",
    "def extract_audio(video_path, audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
    "    \n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    audio = audio.set_frame_rate(16000)\n",
    "    audio.export(audio_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting clip from ../../full_videos/LFB1_full.mp4 to clip.avi\n",
      "Is opened?: True\n",
      "Video clip saved at clip.avi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_vid_path = '../../full_videos/LFB1_full.mp4'\n",
    "video_clip_path = 'clip.avi'\n",
    "audio_clip_path = 'clip.wav'\n",
    "start_secs = 15\n",
    "duration = 20\n",
    "\n",
    "extract_clip_from_full(full_vid_path, video_clip_path, start_secs, duration)\n",
    "extract_audio(video_clip_path, audio_clip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>speaker</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030969</td>\n",
       "      <td>1.009719</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>their criteria.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.009719</td>\n",
       "      <td>3.254094</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>She symptomatically feels fine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.334094</td>\n",
       "      <td>6.089094</td>\n",
       "      <td>SPEAKER_02</td>\n",
       "      <td>What's the criteria now to come back?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.219719</td>\n",
       "      <td>12.636594</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "      <td>I think it's like a week if you're on the 7th ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.092219</td>\n",
       "      <td>13.547844</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "      <td>OK.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start        end     speaker  \\\n",
       "0   0.030969   1.009719  SPEAKER_02   \n",
       "1   1.009719   3.254094  SPEAKER_00   \n",
       "2   4.334094   6.089094  SPEAKER_02   \n",
       "3   7.219719  12.636594  SPEAKER_00   \n",
       "4  13.092219  13.547844  SPEAKER_01   \n",
       "\n",
       "                                       transcription  \n",
       "0                                    their criteria.  \n",
       "1                    She symptomatically feels fine.  \n",
       "2              What's the criteria now to come back?  \n",
       "3  I think it's like a week if you're on the 7th ...  \n",
       "4                                                OK.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = DialogueExtractor(audio_clip_path, transcription_type='api')\n",
    "extractor.extract_dialogue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
