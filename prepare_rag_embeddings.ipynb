{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_transcriptions_dir = 'results/extract_dialogue/aligned_transcriptions/'\n",
    "aligned_fb_detection_dir = 'results/extract_dialogue/aligned_fb_detection/'\n",
    "component_classification_dir = 'results/extract_dialogue/component_classification/'\n",
    "rag_embeddings_dir = 'results/extract_dialogue/rag_embeddings/'\n",
    "no_fb_min_str_len = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embeddings(transcriptions):\n",
    "    fb_idxs = []\n",
    "    no_fb_idxs = []\n",
    "\n",
    "    for i in range(len(transcriptions)):\n",
    "        transcription = str(transcriptions.loc[i, 'transcription'])\n",
    "        if len(eval(transcriptions.loc[i, 'human_annotations'])) > 0:\n",
    "            fb_idxs.append(i)\n",
    "        elif len(transcription) > no_fb_min_str_len:\n",
    "            no_fb_idxs.append(i)\n",
    "    \n",
    "    fb_phrases, fb_times = [], []\n",
    "    no_fb_phrases, no_fb_times = [], []\n",
    "    \n",
    "    for i in fb_idxs:\n",
    "        phrase = transcriptions.loc[i, 'transcription']\n",
    "        start = transcriptions.loc[i, 'start']\n",
    "        end = transcriptions.loc[i, 'end']\n",
    "        start_hms = f'{int(start//3600):02d}:{int((start%3600)//60):02d}:{int(start%60):02d}'\n",
    "        end_hms = f'{int(end//3600):02d}:{int((end%3600)//60):02d}:{int(end%60):02d}'\n",
    "        time = f\"{start_hms}-{end_hms}\"\n",
    "        \n",
    "        fb_phrases.append(phrase)\n",
    "        fb_times.append(time)\n",
    "\n",
    "    for i in no_fb_idxs:\n",
    "        phrase = transcriptions.loc[i, 'transcription']\n",
    "        start = transcriptions.loc[i, 'start']\n",
    "        end = transcriptions.loc[i, 'end']\n",
    "        start_hms = f'{int(start//3600):02d}:{int((start%3600)//60):02d}:{int(start%60):02d}'\n",
    "        end_hms = f'{int(end//3600):02d}:{int((end%3600)//60):02d}:{int(end%60):02d}'\n",
    "        time = f\"{start_hms}-{end_hms}\"\n",
    "        \n",
    "        no_fb_phrases.append(phrase)\n",
    "        no_fb_times.append(time)\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    fb_embeddings = model.encode(fb_phrases)\n",
    "    no_fb_embeddings = model.encode(no_fb_phrases)\n",
    "    \n",
    "    return fb_phrases, no_fb_phrases, fb_times, no_fb_times, fb_embeddings, no_fb_embeddings\n",
    "\n",
    "def run_phrase(case_id):\n",
    "    transcriptions = pd.read_csv(os.path.join(aligned_transcriptions_dir, f'LFB{case_id}_full.csv'), index_col=0)\n",
    "    \n",
    "    dir_ = os.path.join(rag_embeddings_dir, f'phrase_only')\n",
    "    \n",
    "    annotations_dir = os.path.join(dir_, 'annotations')\n",
    "    fb_embeddings_dir = os.path.join(dir_, f'fb')\n",
    "    no_fb_embeddings_dir = os.path.join(dir_, f'no_fb')\n",
    "    \n",
    "    os.makedirs(annotations_dir, exist_ok=True)\n",
    "    os.makedirs(fb_embeddings_dir, exist_ok=True)\n",
    "    os.makedirs(no_fb_embeddings_dir, exist_ok=True)\n",
    "    \n",
    "    fb_phrases, no_fb_phrases, fb_times, no_fb_times, fb_embeddings, no_fb_embeddings = get_phrase_embeddings(transcriptions)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['time', 'case', 'transcription', 'fb_instance', 'embedding_path'])\n",
    "    \n",
    "    for i in range(len(fb_embeddings)):\n",
    "        embedding = fb_embeddings[i]\n",
    "        time = fb_times[i]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        df.loc[len(df)] = [time, case_id, fb_phrases[i], True, path]\n",
    "    \n",
    "    for i in range(len(no_fb_embeddings)):\n",
    "        embedding = no_fb_embeddings[i]\n",
    "        time = no_fb_times[i]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(no_fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        df.loc[len(df)] = [time, case_id, no_fb_phrases[i], False, path]\n",
    "    \n",
    "    df.to_csv(os.path.join(annotations_dir, f'LFB{case_id}_full.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(26, 34):\n",
    "#     print(f'Processing LFB{i}')\n",
    "#     run_phrase(i)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = pd.read_csv('results/extract_dialogue/aligned_transcriptions/LFB26_full.csv')\n",
    "fb_idxs = []\n",
    "no_fb_idxs = []\n",
    "\n",
    "for i in range(len(transcriptions)):\n",
    "    transcription = str(transcriptions.loc[i, 'transcription'])\n",
    "    if len(eval(transcriptions.loc[i, 'human_annotations'])) > 0:\n",
    "        fb_idxs.append(i)\n",
    "    elif len(transcription) > no_fb_min_str_len:\n",
    "        no_fb_idxs.append(i)\n",
    "\n",
    "fb_phrases, fb_times = [], []\n",
    "no_fb_phrases, no_fb_times = [], []\n",
    "\n",
    "for i in fb_idxs:\n",
    "    phrase = transcriptions.loc[i, 'transcription']\n",
    "    start = transcriptions.loc[i, 'start']\n",
    "    end = transcriptions.loc[i, 'end']\n",
    "    start_hms = f'{int(start//3600):02d}:{int((start%3600)//60):02d}:{int(start%60):02d}'\n",
    "    end_hms = f'{int(end//3600):02d}:{int((end%3600)//60):02d}:{int(end%60):02d}'\n",
    "    time = f\"{start_hms}-{end_hms}\"\n",
    "    \n",
    "    fb_phrases.append(phrase)\n",
    "    fb_times.append(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context + Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_phrase_embeddings_fb(aligned_fb_detection):\n",
    "    fb_df = aligned_fb_detection[aligned_fb_detection['true_fb_instance'] == True]\n",
    "    no_fb_df = aligned_fb_detection[aligned_fb_detection['true_fb_instance'] == False]\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    fb_embeddings = model.encode(fb_df['context_dialogue'].to_list())\n",
    "    no_fb_embeddings = model.encode(no_fb_df['context_dialogue'].to_list())\n",
    "    \n",
    "    return fb_embeddings, no_fb_embeddings, fb_df, no_fb_df\n",
    "\n",
    "def get_context_phrase_embeddings_component(component_classification):\n",
    "    anatomic_df = component_classification[component_classification['true_f_anatomic'] == True]\n",
    "    procedural_df = component_classification[component_classification['true_f_procedural'] == True]\n",
    "    technical_df = component_classification[component_classification['true_f_technical'] == True]\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    anatomic_embeddings = model.encode(anatomic_df['context_dialogue'].to_list())\n",
    "    procedural_embeddings = model.encode(procedural_df['context_dialogue'].to_list())\n",
    "    technical_embeddings = model.encode(technical_df['context_dialogue'].to_list())\n",
    "    \n",
    "    return anatomic_embeddings, procedural_embeddings, technical_embeddings, anatomic_df, procedural_df, technical_df\n",
    "    \n",
    "def run_context_phrase(case_id):\n",
    "    aligned_fb_detection = pd.read_csv(os.path.join(aligned_fb_detection_dir, f\"LFB{case_id}_full 'all phrases'.csv\"))\n",
    "    component_classification = pd.read_csv(os.path.join(component_classification_dir, f\"LFB{case_id}_full 'all phrases'.csv\"))\n",
    "    \n",
    "    dir_ = os.path.join(rag_embeddings_dir, f'context+phrase')\n",
    "    \n",
    "    annotations_fb_dir = os.path.join(dir_, 'annotations_fb')\n",
    "    annotations_component_dir = os.path.join(dir_, 'annotations_component')\n",
    "    fb_embeddings_dir = os.path.join(dir_, f'fb')\n",
    "    no_fb_embeddings_dir = os.path.join(dir_, f'no_fb')\n",
    "    \n",
    "    os.makedirs(annotations_fb_dir, exist_ok=True)\n",
    "    os.makedirs(annotations_component_dir, exist_ok=True)\n",
    "    os.makedirs(fb_embeddings_dir, exist_ok=True)\n",
    "    os.makedirs(no_fb_embeddings_dir, exist_ok=True)\n",
    "    \n",
    "    fb_embeddings, no_fb_embeddings, fb_df, no_fb_df = get_context_phrase_embeddings_fb(aligned_fb_detection)\n",
    "    f_anatomic_embeddings, f_procedural_embeddings, f_technical_embeddings, f_anatomic_df, f_procedural_df, f_technical_df = get_context_phrase_embeddings_component(component_classification)\n",
    "    \n",
    "    annotations_fb = pd.DataFrame(columns=['time', 'case_id', 'context_dialogue', 'fb_instance', 'embedding_path'])\n",
    "    for i in range(len(fb_embeddings)):\n",
    "        embedding = fb_embeddings[i]\n",
    "        time = fb_df.iloc[i]['phrase'][1:18]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        # context_dialogue = fb_df.iloc[i]['context_dialogue'].split('\\n')[1:-1]\n",
    "        # context_dialogue = [x.split(':')[2][:-1].strip() for x in context_dialogue]\n",
    "        # context_dialogue = ' '.join(context_dialogue)\n",
    "        context_dialogue = fb_df.iloc[i]['context_dialogue']\n",
    "        \n",
    "        annotations_fb.loc[len(annotations_fb)] = [time, case_id, context_dialogue, True, path]\n",
    "    for i in range(len(no_fb_embeddings)):\n",
    "        embedding = no_fb_embeddings[i]\n",
    "        time = no_fb_df.iloc[i]['phrase'][1:18]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(no_fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        # context_dialogue = no_fb_df.iloc[i]['context_dialogue'].split('\\n')[1:-1]\n",
    "        # context_dialogue = [x.split(':')[2].strip() for x in context_dialogue]\n",
    "        # context_dialogue = ' '.join(context_dialogue)\n",
    "        context_dialogue = no_fb_df.iloc[i]['context_dialogue']\n",
    "\n",
    "        annotations_fb.loc[len(annotations_fb)] = [time, case_id, context_dialogue, False, path]\n",
    "    annotations_fb.to_csv(os.path.join(annotations_fb_dir, f'LFB{case_id}_full.csv'))\n",
    "\n",
    "    \n",
    "    annotations_component = pd.DataFrame(columns=['time', 'case_id', 'context_dialogue', 'f_anatomic', 'f_procedural', 'f_technical', 'embedding_path'])\n",
    "    for i in range(len(f_anatomic_embeddings)):\n",
    "        embedding = f_anatomic_embeddings[i]\n",
    "        time = f_anatomic_df.iloc[i]['phrase'][1:18]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        # context_dialogue = f_anatomic_df.iloc[i]['context_dialogue'].split('\\n')[1:-1]\n",
    "        # context_dialogue = [x.split(':')[2][:-1].strip() for x in context_dialogue]\n",
    "        # context_dialogue = ' '.join(context_dialogue)\n",
    "        context_dialogue = f_anatomic_df.iloc[i]['context_dialogue']\n",
    "        \n",
    "        f_anatomic, f_procedural, f_technical = f_anatomic_df.iloc[i]['true_f_anatomic'], f_anatomic_df.iloc[i]['true_f_procedural'], f_anatomic_df.iloc[i]['true_f_technical']\n",
    "        \n",
    "        if time not in annotations_component['time']:\n",
    "            annotations_component.loc[len(annotations_component)] = [time, case_id, context_dialogue, f_anatomic, f_procedural, f_technical, path]\n",
    "    for i in range(len(f_procedural_embeddings)):\n",
    "        embedding = f_procedural_embeddings[i]\n",
    "        time = f_procedural_df.iloc[i]['phrase'][1:18]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        # context_dialogue = f_procedural_df.iloc[i]['context_dialogue'].split('\\n')[1:-1]\n",
    "        # context_dialogue = [x.split(':')[2].strip() for x in context_dialogue]\n",
    "        # context_dialogue = ' '.join(context_dialogue)\n",
    "        context_dialogue = f_procedural_df.iloc[i]['context_dialogue']\n",
    "        \n",
    "        f_anatomic, f_procedural, f_technical = f_procedural_df.iloc[i]['true_f_anatomic'], f_procedural_df.iloc[i]['true_f_procedural'], f_procedural_df.iloc[i]['true_f_technical']\n",
    "        \n",
    "        if time not in annotations_component['time']:\n",
    "            annotations_component.loc[len(annotations_component)] = [time, case_id, context_dialogue, f_anatomic, f_procedural, f_technical, path]\n",
    "    for i in range(len(f_technical_embeddings)):\n",
    "        embedding = f_technical_embeddings[i]\n",
    "        time = f_technical_df.iloc[i]['phrase'][1:18]\n",
    "        filename = f'LFB{case_id}_{time}.npy'\n",
    "        path = os.path.join(fb_embeddings_dir, filename)\n",
    "        np.save(path, embedding)\n",
    "\n",
    "        # context_dialogue = f_technical_df.iloc[i]['context_dialogue'].split('\\n')[1:-1]\n",
    "        # context_dialogue = [x.split(':')[2].strip() for x in context_dialogue]\n",
    "        # context_dialogue = ' '.join(context_dialogue)\n",
    "        context_dialogue = f_technical_df.iloc[i]['context_dialogue']\n",
    "        \n",
    "        f_anatomic, f_procedural, f_technical = f_technical_df.iloc[i]['true_f_anatomic'], f_technical_df.iloc[i]['true_f_procedural'], f_technical_df.iloc[i]['true_f_technical']\n",
    "        \n",
    "        if time not in annotations_component['time']:\n",
    "            annotations_component.loc[len(annotations_component)] = [time, case_id, context_dialogue, f_anatomic, f_procedural, f_technical, path]\n",
    "    annotations_component.to_csv(os.path.join(annotations_component_dir, f'LFB{case_id}_full.csv'))\n",
    "    \n",
    "    return annotations_fb, annotations_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:35:37-00:35:40'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_fb_detection = pd.read_csv(os.path.join(aligned_fb_detection_dir, f\"LFB{1}_full 'all phrases'.csv\"))\n",
    "aligned_fb_detection.iloc[0]['phrase'][1:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LFB1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firdavs/surgery/firdavs_work/.venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing LFB2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firdavs/surgery/firdavs_work/.venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing LFB6\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB6_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB8\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB8_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firdavs/surgery/firdavs_work/.venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing LFB10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firdavs/surgery/firdavs_work/.venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing LFB11\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB11_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB12\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB12_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB13\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB13_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB15\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB15_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB16\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB16_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB17\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB17_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/firdavs/surgery/firdavs_work/.venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing LFB19\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB19_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB20\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB20_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB21\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB21_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB22\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB22_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB23\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB23_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB24\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/aligned_fb_detection/LFB24_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB25\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB25_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB26\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB26_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB27\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/aligned_fb_detection/LFB27_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB28\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB28_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB29\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB29_full 'all phrases'.csv\"\n",
      "\n",
      "Processing LFB33\n",
      "Error: [Errno 2] No such file or directory: \"results/extract_dialogue/component_classification/LFB33_full 'all phrases'.csv\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 33]:\n",
    "    print(f'Processing LFB{i}')\n",
    "    try:\n",
    "        run_context_phrase(i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
